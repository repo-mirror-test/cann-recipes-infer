model_name: "deepseek_v2_lite"
model_path: "/data/models/origin/DeepSeek-V2-Lite/" # download from "https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite"
exe_mode: "ge_graph"                # ["ge_graph", "acl_graph", "eager"], mode of decode
world_size: 32

model_config:
  mm_quant_mode: A16W16             # ["A16W16", "A8W8"]
  gmm_quant_mode: A16W16            # ["A16W16", "A8W8"]
  enable_pa: False                  # [False, True]
  pa_block_size: 128
  enable_weight_nz: False
  enable_mla_prolog: False
  with_ckpt: True                   # [False, True]
  enable_multi_streams: True        # [False, True]
  enable_profiler: False            # [False, True]
  perfect_eplb: False               # [False, True]
  enable_cache_compile: False       # [False, True]
  enable_prefill_multi_cycle: False # [False, True]
  enable_superkernel: False         # [False, True]
  enable_online_split_weight: True   # [False, True]
  moe_chunk_max_len: 65536
  micro_batch_mode: 0               # [0, 1]

data_config:
  dataset: "default"                # ["default", "LongBench"]
  input_max_len: 32
  max_new_tokens: 100
  batch_size: 256 # global batch_size, should be divisible by ep_size

parallel_config:
  attn_tp_size: 1 # attn_dp_size = world_size // attn_tp_size
  dense_tp_size: 1
  moe_tp_size: 1 # moe_dp_size = world_size // moe_tp_size
  embed_tp_size: 1
  lmhead_tp_size: 1
